{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02753eb8-f905-4614-893f-2e18e749370f",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories For Topics On GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcbbbd-4df6-4a84-8c47-ab32ec9b7336",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f03931-bde5-4f87-9948-6006933f62c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Web scraping\n",
    "- Web scraping is an automated method used to extract data from websites. It involves fetching a webpage, parsing its content, and retrieving specific information using tools like requests for HTTP requests and BeautifulSoup or Scrapy for HTML parsing. Web scraping is commonly used for data mining, price monitoring, competitive analysis, research, and automation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729987b3-db02-4285-b2cc-ec3e14fa226c",
   "metadata": {},
   "source": [
    "### GitHub and the problem statement\n",
    "- GitHub is a web-based platform for version control and collaboration that allows developers to manage and share their code using Git. It provides features like repositories, branches, pull requests, issue tracking, and CI/CD integration, making it a key tool for open-source and professional software development.\n",
    "- In this project, we are extracting data about the top repositories for the top topics of GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5efe12-a2fd-4234-a816-68bf3e9f2fd4",
   "metadata": {},
   "source": [
    "### Tools used\n",
    "- Python, Pandas, BeautifulSoup, requests, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a43c35-9f5d-4a49-a5cb-dc6302a99a36",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ab229-0777-4fb3-90e6-ce94283939b9",
   "metadata": {},
   "source": [
    "1. Importing all the necessary libraries\n",
    "2. Scraping ***https://github.com/topics*** to extract top topics' name, description and page URL.\n",
    "3. Store the extracted data in a DataFrame.\n",
    "4. For each topic, scraping the top 20-25 repositories from the topic page to extract username, repo name, stars and repo URL.\n",
    "5. Storing the data extracted for each topic into a CSV file in the below format:\n",
    "\n",
    "    *Repo Name,Username,Stars,Repo URL\n",
    "    <br>three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "    <br>libgdx,libgdx,18300,https://github.com/libgdx/libgdx*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb751c1-02ad-4cfd-b4b4-91ec768da3c9",
   "metadata": {},
   "source": [
    "## Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b3b1b1-67f1-47d9-a9d0-7b84e645e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b57066c-81e7-455d-a2b5-d8089f2f62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://github.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d97a7a-6c8f-4cee-9751-8d35b38a8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "timest = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logs_dir = 'LOGS'\n",
    "\n",
    "os.makedirs(logs_dir,exist_ok=True)\n",
    "log_filename = os.path.join(logs_dir, f\"github_scraping_{timest}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename = log_filename,\n",
    "    level= logging.INFO,\n",
    "    format = \"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt = \"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc35c1-8a6e-497c-ae53-238613da4cfa",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90e8d2-4706-4c2e-8f4a-e1d9d0b3aecb",
   "metadata": {},
   "source": [
    "### Defining function to fetch web page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d115ed-1f8a-4803-be4a-7920ca5983bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(url):\n",
    "    \"\"\"Fetches the response from a given URL.\"\"\"\n",
    "    logging.info(f\"Fetching URL : {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200: # Check if the request was successful (HTTP 200 OK)\n",
    "            logging.error(f\"Failed to load page: {url}. Status Code:{response.status_code}\")\n",
    "            raise Exception(f'Failed to load page: {url}. Status Code:{response.status_code}')\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.critical(f\"Request error for {url}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e0318-9312-4f66-a085-143d97af1fcc",
   "metadata": {},
   "source": [
    "### Defining function to to parse HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9d8c31-88a3-48d4-bd19-a3b625674476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(res):\n",
    "    \"\"\"Parses the HTML content from an HTTP response using BeautifulSoup.\"\"\"\n",
    "    \n",
    "    logging.info(\"Parsing HTML content with BeautifulSoup\")\n",
    "    try:\n",
    "        page_content = res.text # Extract raw HTML content\n",
    "        if not page_content:\n",
    "            logging.warning(\"Warning: The response content is empty.\")\n",
    "        soup = BeautifulSoup(page_content, 'html.parser') # Parse HTML with BeautifulSoup\n",
    "        logging.info(\"Successfully parsed HTML content.\")\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while parsing HTML: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf695f69-e5cd-470c-bb41-d00b3b4dc558",
   "metadata": {},
   "source": [
    "### Defining function to extract topic name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "862294d2-e5ed-4157-9cd1-8c57709f9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_name(topic):\n",
    "    \"\"\"Extracts the topic name from a given BeautifulSoup element.\"\"\"\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'  # Class used to identify topic title\n",
    "    logging.info(\"Extracting topic name...\")\n",
    "    topic_title_tag = topic.find('p',{'class': selection_class}) # Locate the <p> tag containing the topic name\n",
    "    if topic_title_tag:\n",
    "         # Extract and return the text content of the topic\n",
    "            topic_name = topic_title_tag.get_text().strip()\n",
    "            logging.info(f\"Extracted topic name: {topic_name}\")\n",
    "            return topic_name\n",
    "    logging.warning(\"Topic name not found!\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a282a7-afd6-4514-a478-5ae3b3bfb1a2",
   "metadata": {},
   "source": [
    "### Defining function to extract topic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a17b83a-f8fa-4a62-9cac-31c4b9b063d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_desc(topic):\n",
    "    \"\"\"Extracts the topic description from a given BeautifulSoup element.\"\"\"\n",
    "    selection_class = 'f5 color-fg-muted mb-0 mt-1' # Class used to identify topic description\n",
    "    logging.info(\"Extracting topic description...\")\n",
    "    topic_desc_tag  = topic.find('p', {'class': selection_class}) # Locate the <p> tag containing the description\n",
    "    if topic_desc_tag:\n",
    "        topic_desc = topic_desc_tag.get_text().strip()  # Extract and clean the topic description\n",
    "        logging.info(f\"Extracted topic description: {topic_desc[:50]}...\")  # Log first 50 chars\n",
    "        return topic_desc\n",
    "    logging.warning(\"Topic description not found!\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf4ec6-d531-4b6a-b40d-ddf3fee161c1",
   "metadata": {},
   "source": [
    "### Defining function to extract topic URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4d9215-b48d-4fb4-91f9-c1551dd21108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_url(topic):\n",
    "    \"\"\"Extracts the full URL of a GitHub topic from a given BeautifulSoup element.\"\"\"\n",
    "    global BASE_URL # Using the global BASE_URL to construct the full URL\n",
    "    selection_class = 'no-underline flex-1 d-flex flex-column' # Class used to identify topic link\n",
    "    logging.info(\"Extracting topic URL...\")\n",
    "    topic_url_tag = topic.find('a', {'class': selection_class}) # Locate the <a> tag containing the URL\n",
    "    if topic_url_tag:\n",
    "        try:\n",
    "            topic_url = f\"{BASE_URL}{topic_url_tag['href']}\"  # Construct full topic URL\n",
    "            logging.info(f\"Successfully extracted topic URL: {topic_url}\")\n",
    "            return topic_url\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Encountered error while extracting topic url: {e}\")\n",
    "            raise\n",
    "    logging.warning(\"Topic URL not found!\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4275d-75a6-4322-ba8b-f2472bcee123",
   "metadata": {},
   "source": [
    "### Defining function to extract GitHub Topics Data and store in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8161c9dd-f20a-4918-b10e-956b8702ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_data(soup):\n",
    "    \"\"\"Extracts GitHub topics (name, description, and URL) from a BeautifulSoup object.\"\"\"\n",
    "    global BASE_URL # Using a global base URL\n",
    "    topics_dict = dict() # Dictionary to store topic details\n",
    "    topic_name = topic_desc = topic_url = \"\"\n",
    "    \n",
    "    # Find all div tags that contain topic information\n",
    "    logging.info(\"Extracting GitHub topics from the page...\")\n",
    "    topics_div = soup.find_all('div',{'class':'py-4 border-bottom d-flex flex-justify-between'})\n",
    "\n",
    "    if not topics_div:\n",
    "        logging.warning(\"No topics found on the page.\")  # Log if no topics are found\n",
    "        return topics_dict\n",
    "    \n",
    "    # Loop through each topic div to extract relevant data\n",
    "    for topic in topics_div:\n",
    "        # getting title, description and url\n",
    "        topic_name  = get_topic_name(topic) # Extract topic name\n",
    "        topic_desc  = get_topic_desc(topic) # Extract topic description\n",
    "        topic_url   = get_topic_url(topic)  # Extract topic URL\n",
    "        \n",
    "        if topic_name:\n",
    "            topics_dict[topic_name] = {\n",
    "                'topic_name': topic_name,\n",
    "                'topic_description': topic_desc,\n",
    "                'topic_url': topic_url\n",
    "            }\n",
    "            logging.info(f\"Extracted data for topic: {topic_name}\")\n",
    "        else:\n",
    "            logging.warning(\"A topic was found without a name. Skipping entry.\")\n",
    "            continue\n",
    "    logging.info(f\"Total topics extracted: {len(topics_dict)}\")\n",
    "    return topics_dict # Return the dictionary containing topic details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818756f-bc56-48e3-acf2-5ad83fe6c1f5",
   "metadata": {},
   "source": [
    "### Defining function to create a DataFrame from Topics Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7211287a-6122-48ee-b799-15646c7aa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics_dataframe(topics_dict):\n",
    "    \"\"\"Converts a dictionary of GitHub topics into a Pandas DataFrame.\"\"\"\n",
    "\n",
    "    # Convert dictionary to DataFrame, transpose to match correct format\n",
    "    logging.info(\"Converting topics dictionary into a Pandas DataFrame...\")\n",
    "    if not topics_dict:  # Check if the dictionary is empty\n",
    "        logging.warning(\"The topics dictionary is empty. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['topic_name', 'topic_description', 'topic_url'])\n",
    "\n",
    "    df = pd.DataFrame(topics_dict).transpose().reset_index(drop=True)\n",
    "    df.columns = ['topic_name', 'topic_description', 'topic_url']\n",
    "    logging.info(f\"Successfully created DataFrame with {df.shape[0]} topics.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c3158-859b-46ce-be8a-fcd29e505ca8",
   "metadata": {},
   "source": [
    "### Defining function to extract repository information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101c4da9-7662-4600-a7d0-55501d92b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(repo):\n",
    "    \"\"\"Extracts repository details (username, repo name, URL, and stars) from a BeautifulSoup element.\"\"\"\n",
    "    username = repo_name = repo_url = \"\"\n",
    "    selection_class = 'f3 color-fg-muted text-normal lh-condensed' # Class identifying repo owner and name\n",
    "\n",
    "    logging.info(\"Extracting repository information...\")\n",
    "    try:\n",
    "        username_tag = repo.find('h3',{'class': selection_class}) # Find the <h3> tag containing repo details\n",
    "        if username_tag:\n",
    "            a_tags = username_tag.find_all('a') # Extract all <a> tags\n",
    "            if len(a_tags)>=2: # Ensure there are at least two <a> tags (username and repo name)\n",
    "                username  = a_tags[0].get_text().strip()\n",
    "                repo_name = a_tags[1].get_text().strip()\n",
    "                repo_url  = f\"{BASE_URL}{a_tags[1]['href']}\" # Construct full repository URL\n",
    "                logging.info(f\"Extracted repo: {repo_name} by {username}\")\n",
    "        # Find the repository star count\n",
    "        star_tag = repo.find('span',{'id':'repo-stars-counter-star'})\n",
    "        if star_tag:\n",
    "            stars = star_tag.get_text().strip()\n",
    "        if not username or not repo_name or not repo_url or not stars:\n",
    "            logging.warning(\"Some repository details are missing.\")      \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting repository information: {e}\")\n",
    "    finally:\n",
    "        return username, repo_name, repo_url, stars     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e5ed4-5565-47bb-bc3b-0e4dd291b571",
   "metadata": {},
   "source": [
    "### Defining function to extract all the repositories from a GitHub topic age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25898aaa-5edc-45d8-90b6-c255466c2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_repos(soup):\n",
    "    \"\"\"Extracts all repository elements from a GitHub topic page using BeautifulSoup.\"\"\"\n",
    "    selection_class = 'border rounded color-shadow-small color-bg-subtle my-4' # Class identifying repository sections\n",
    "\n",
    "    logging.info(\"Extracting all repository elements from the GitHub topic page...\")\n",
    "    repos = soup.find_all('article',{'class':selection_class}) # Extract all repository elements\n",
    "\n",
    "    if not repos:\n",
    "        logging.warning(\"No repositories found on the topic page.\")  # Log warning if no repositories found\n",
    "    else:\n",
    "        logging.info(f\"Successfully extracted {len(repos)} repositories.\")\n",
    "    return repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bb8d3-6176-48fa-982c-b3cfe3f5d359",
   "metadata": {},
   "source": [
    "### Defining function to scrape repository data from a GitHub Topic Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6b6d649-ce33-4551-9a2a-d8c567d5bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_repo(url):\n",
    "    \"\"\"Scrapes repository details (name, owner, stars, URL) from a GitHub topic page.\"\"\"\n",
    "    logging.info(f\"Starting repository scraping for: {url}\")\n",
    "    repo_dict = dict() # Dictionary to store repository data\n",
    "\n",
    "    try:\n",
    "        response = get_response(url) # Fetch the topic page content\n",
    "        soup = get_soup(response) # Parse the page using BeautifulSoup\n",
    "        repos = get_all_repos(soup) # Extract repository elements\n",
    "        \n",
    "        if not repos:\n",
    "            logging.warning(f\"No repositories found at {url}.\")\n",
    "            return repo_dict\n",
    "            \n",
    "        # Loop through each repository element and extract details\n",
    "        for repo in repos:\n",
    "            username, repo_name, repo_url, stars = get_repo_info(repo)\n",
    "            # Store extracted details\n",
    "            if username:\n",
    "                repo_dict[username] = {  # Store extracted details\n",
    "                    'repo_name': repo_name,\n",
    "                    'stars': stars,\n",
    "                    'repo_url': repo_url\n",
    "                }\n",
    "                logging.info(f\"Extracted repository: {repo_name} by {username}\")\n",
    "        logging.info(f\"Successfully scraped {len(repo_dict)} repositories from {url}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while scraping repositories from {url}: {e}\")\n",
    "    finally:\n",
    "        return repo_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ef1b1-54f0-4061-b0a4-cf8c5ecd8685",
   "metadata": {},
   "source": [
    "### Defining function for scraping GitHub Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d281870a-1583-4e2d-a521-ecb6c1040a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\"Scrapes GitHub Topics, extracts details, and returns a DataFrame.\"\"\"\n",
    "    topics_url = \"https://github.com/topics\" # URL of GitHub Topics page\n",
    "\n",
    "    logging.info(f\"Starting to scrape GitHub topics from {topics_url}\")\n",
    "    try:\n",
    "        response = get_response(topics_url) # Fetch page content\n",
    "        soup = get_soup(response) # Parse HTML using BeautifulSoup\n",
    "        topics_dict = get_topics_data(soup) \n",
    "        if not topics_dict:  # Check if any topics were extracted\n",
    "            logging.warning(\"No topics were extracted. Returning an empty DataFrame.\")\n",
    "            return pd.DataFrame(columns=['topic_name', 'topic_description', 'topic_url'])\n",
    "        topics_df = create_topics_dataframe(topics_dict)  # Convert to DataFrame\n",
    "        logging.info(f\"Successfully scraped {len(topics_df)} topics.\")\n",
    "        return topics_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while scraping GitHub topics: {e}\")\n",
    "        return pd.DataFrame(columns=['topic_name', 'topic_description', 'topic_url']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c668e9-ab16-431b-8ff7-f567da32517f",
   "metadata": {},
   "source": [
    "# Scraping GitHub Topics and Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2ea9e5-a567-4be3-a163-4a92cb9f9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repo():\n",
    "    \"\"\"Scrapes GitHub topics and their repositories, saving data into CSV files.\"\"\"\n",
    "\n",
    "    logging.info(\"Starting GitHub topic and repository scraping process.\")\n",
    "    \n",
    "    topics_df = scrape_topics() # Fetch GitHub topics\n",
    "    if topics_df.empty:  # Check if no topics were extracted\n",
    "        logging.warning(\"No topics found. Exiting repository scraping process.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    for topic_url in topics_df['topic_url']:\n",
    "        topic_name = topic_url.split('/')[-1] # Extract topic name from URL\n",
    "        logging.info(f\"Scraping top repositories for: {topic_name}\")\n",
    "        repo_dict = scrape_repo(topic_url) # Scrape repositories for the topic\n",
    "        \n",
    "        if not repo_dict:  # Check if no repositories were found\n",
    "            logging.warning(f\"No repositories found for {topic_name}. Skipping CSV creation.\")\n",
    "            continue\n",
    "        \n",
    "        repo_df = pd.DataFrame.from_dict(repo_dict).transpose() # Convert to DataFrame\n",
    "        \n",
    "        # Format DataFrame: reset index and rename columns\n",
    "        repo_df.reset_index(inplace=True)\n",
    "        repo_df.rename(columns={'index':'username'},inplace=True)\n",
    "        \n",
    "        # Ensure 'Data' directory exists\n",
    "        os.makedirs('Data',exist_ok=True)\n",
    "        file_path = Path(\"Data\") / f\"{topic_name}.csv\" # Define CSV file path\n",
    "        \n",
    "        if file_path.exists(): # Check if file already exists\n",
    "            logging.info(f\"File {file_path} already exists. Skipping...\")\n",
    "            continue\n",
    "        try:\n",
    "            repo_df.to_csv(file_path, index=False)  # Save DataFrame to CSV\n",
    "            logging.info(f\"Successfully saved repositories for {topic_name} to {file_path}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e2d611b-d5f5-4248-b688-5417c7c430ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topics_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bacd5e-2e29-4d21-ac85-e6afe9456292",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1701ba6-978a-4409-b81b-8a27ec628540",
   "metadata": {},
   "source": [
    "This project successfully **scrapes GitHub Topics** and their **top repositories**, extracting key details and storing them in CSV files.  \n",
    "\n",
    "### **What This Notebook Does**\n",
    "- Fetches **GitHub Topics** from the GitHub Topics page.\n",
    "- Extracts **topic details** (name, description, URL).\n",
    "- Scrapes **top repositories** under each topic.\n",
    "- Extracts **repository details** (Owner, Name, Stars, URL).\n",
    "- Saves the collected data into **CSV files** for analysis.\n",
    "- Implements **logging** to track scraping progress and handle errors.\n",
    "\n",
    "### **Data Storage**\n",
    "- Each topic’s repositories are saved in **Data/topic_name.csv**.\n",
    "- Example:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17eec8b4-19f4-4425-ac1a-f8f4a43bf963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>stars</th>\n",
       "      <th>repo_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mrdoob</td>\n",
       "      <td>three.js</td>\n",
       "      <td>105k</td>\n",
       "      <td>https://github.com/mrdoob/three.js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pmndrs</td>\n",
       "      <td>react-three-fiber</td>\n",
       "      <td>28.3k</td>\n",
       "      <td>https://github.com/pmndrs/react-three-fiber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>libgdx</td>\n",
       "      <td>libgdx</td>\n",
       "      <td>23.8k</td>\n",
       "      <td>https://github.com/libgdx/libgdx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BabylonJS</td>\n",
       "      <td>Babylon.js</td>\n",
       "      <td>23.7k</td>\n",
       "      <td>https://github.com/BabylonJS/Babylon.js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FreeCAD</td>\n",
       "      <td>FreeCAD</td>\n",
       "      <td>23.3k</td>\n",
       "      <td>https://github.com/FreeCAD/FreeCAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    username          repo_name  stars  \\\n",
       "0     mrdoob           three.js   105k   \n",
       "1     pmndrs  react-three-fiber  28.3k   \n",
       "2     libgdx             libgdx  23.8k   \n",
       "3  BabylonJS         Babylon.js  23.7k   \n",
       "4    FreeCAD            FreeCAD  23.3k   \n",
       "\n",
       "                                      repo_url  \n",
       "0           https://github.com/mrdoob/three.js  \n",
       "1  https://github.com/pmndrs/react-three-fiber  \n",
       "2             https://github.com/libgdx/libgdx  \n",
       "3      https://github.com/BabylonJS/Babylon.js  \n",
       "4           https://github.com/FreeCAD/FreeCAD  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Data/3d.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb8b4f-0c39-4adc-b88e-ff57a312b5ba",
   "metadata": {},
   "source": [
    "### **Future Work**\n",
    "- **Further analyze CSV data** using **Pandas, SQL, or visualization tools**.\n",
    "- **Improve scraping speed** using parallel processing.\n",
    "- **Add database storage** for structured data retrieval.\n",
    "- **Extract additional repository details** (Forks, Issues, Contributors).\n",
    "- **Implement pagination** to scrape **more topics and repositories** by iterating through multiple pages (`?page=1`, `?page=2`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56c008-42c1-492c-b791-3cffea94df4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
